{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAKESPEARE_PATH    = '../data/Shakespeare'\n",
    "JANE_PATH           = '../data/JaneAusten'\n",
    "LOVECRAFT_PATH      = '../data/Lovecraft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalices and returns a sentence.\n",
    "\n",
    "    Note: Taken and adapeted from P06_word2Vec.ipynb\n",
    "    class notebook.\n",
    "    \"\"\"\n",
    "    # Step 1: Remove special chars\n",
    "    sentence = re.sub(r'\\W', ' ', str(sentence))\n",
    "    # Step 2: Remove single characters\n",
    "    sentence = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sentence)\n",
    "    sentence = re.sub(r'\\^[a-zA-Z]\\s+', ' ', sentence)\n",
    "    # Step 3: Remove numbers\n",
    "    sentence = re.sub(r'[0-9]+', ' ', sentence)\n",
    "    # Step 4: remove consecutive spaces\n",
    "    sentence = re.sub(' +', ' ', sentence)\n",
    "    # Step 5: Sentence to lower cases\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_book(path: str) -> str:\n",
    "    book_str = \"\"\n",
    "    book_paths = os.listdir(path)\n",
    "    for book_path in book_paths:\n",
    "        book = open(path + '/' + book_path, 'r', encoding='utf-8')\n",
    "        lines = book.readlines()\n",
    "        for sentence in lines:\n",
    "            book_str += sentence\n",
    "\n",
    "    return book_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'by', 'jane', 'austen', 'chapter', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', 'and', 'had', 'lived', 'nearly', 'twenty', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', 'she', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'most', 'affectionate', 'indulgent', 'father', 'and', 'had', 'in', 'consequence', 'of', 'her', 'sister', 'marriage', 'been', 'mistress', 'of', 'his', 'house', 'from', 'very', 'early', 'period', 'her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance', 'of', 'her', 'caresses', 'and', 'her', 'place', 'had', 'been', 'supplied', 'by', 'an', 'excellent', 'woman', 'as', 'governess', 'who', 'had', 'fallen', 'little', 'short', 'of', 'mother', 'in', 'affection', 'sixteen', 'years', 'had', 'miss', 'taylor', 'been', 'in', 'mr', 'woodhouse', 'family', 'less', 'as', 'governess', 'than', 'friend', 'very', 'fond', 'of', 'both', 'daughters', 'but', 'particularly', 'of', 'emma', 'between', '_them_', 'it', 'was', 'more', 'the', 'intimacy', 'of', 'sisters', 'even', 'before']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "books = {\n",
    "    \"jane\": load_book(JANE_PATH),\n",
    "    \"lovecraft\": load_book(LOVECRAFT_PATH),\n",
    "    \"shakespeare\": load_book(SHAKESPEARE_PATH),\n",
    "}\n",
    "\n",
    "corpus_arr = {\"full\": []}\n",
    "\n",
    "for author in books:\n",
    "    books[author] = normalize(books[author])\n",
    "    tokenized_book = books[author].split()\n",
    "    complete_sentences = len(tokenized_book) // 150\n",
    "    remaining_tokens = len(tokenized_book) % 150\n",
    "    corpus_arr[author] = []\n",
    "    for i in range(complete_sentences):\n",
    "        start = 150 * i\n",
    "        end = start + 150\n",
    "        sentence = tokenized_book[start:end]\n",
    "        corpus_arr[author].append(sentence)\n",
    "        corpus_arr[\"full\"].append(sentence)\n",
    "    \n",
    "    # if remaining_tokens > 0:\n",
    "    #     start = 150 * complete_sentences\n",
    "    #     end = start + remaining_tokens\n",
    "    #     sentence = tokenized_book[start:end]\n",
    "    #     corpus_arr[author].append(sentence)\n",
    "    #     corpus_arr[\"full\"].append(sentence)\n",
    "\n",
    "print(corpus_arr[\"full\"][0])\n",
    "\n",
    "# for author in books:\n",
    "#     corpus[author] = [\"\" for sentence in corpus_arr[author]]\n",
    "#     for i in range(len(corpus[author])):\n",
    "#         sentence = corpus_arr[author][i]\n",
    "#         for token in sentence:\n",
    "#             corpus[author][i] += token + \" \"\n",
    "#         corpus[\"full\"].append(corpus[author][i])\n",
    "\n",
    "# corpus[\"full\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5591"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_arr[\"full\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24044"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "tokens = {}\n",
    "for sentence in corpus_arr[\"full\"]:\n",
    "    for token in sentence:\n",
    "        tokens[token] = 1\n",
    "\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción del embedding de tamaño 50 con su modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "\n",
    "def load_embeding(path: str, sentences_arr: list, size: int = None) -> gensim.models.Word2Vec:\n",
    "    \"\"\"\n",
    "    If embeding path exists, loads and returns the embeding.\n",
    "    If embeding path does not exist, creates, saves and\n",
    "    returns the embeding. If size is not specified, 50\n",
    "    will be used by default.\n",
    "    \"\"\"    \n",
    "    if not os.path.exists(path):\n",
    "        # Give size its default value\n",
    "        if size is None:\n",
    "            size = 50\n",
    "        \n",
    "        # Train enbeding\n",
    "        embeding = gensim.models.Word2Vec(sentences_arr, vector_size=size, window=3, min_count=0, workers=10)\n",
    "        embeding.train(sentences_arr, total_examples=len(sentences_arr), epochs=30)\n",
    "        embeding.save(path)\n",
    "        print(\"Model created and saved\")\n",
    "    else:\n",
    "        embeding = gensim.models.Word2Vec.load(path)\n",
    "        print(\"Model loaded\")\n",
    "\n",
    "    return embeding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_50_PATH  =    \"../models/books_50_l.rojasb_j.arboleda.model\"\n",
    "EMBEDDING_100_PATH =    \"../models/books_100_l.rojasb_j.arboleda.model\"\n",
    "EMBEDDING_150_PATH =    \"../models/books_150_l.rojasb_j.arboleda.model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created and saved\n"
     ]
    }
   ],
   "source": [
    "embeding_50 = load_embeding(EMBEDDING_50_PATH, corpus_arr[\"full\"], 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24044"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeding_50.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def get_tokenizer_from_embeding(embeding: gensim.models.Word2Vec) -> Tokenizer:\n",
    "    vocab = {}\n",
    "    i = 0\n",
    "    for word in embeding.wv.index_to_key:\n",
    "        vocab[word] = i\n",
    "        i += 1\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=len(vocab))\n",
    "    tokenizer.word_index = vocab\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_50 = get_tokenizer_from_embeding(embeding_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus2sequences(corpus: dict, tokenizer: Tokenizer) -> dict:\n",
    "    sequences = {}\n",
    "    for key in corpus:\n",
    "        sequences[key] = tokenizer.texts_to_sequences(corpus[key])\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences_50 = corpus2sequences(corpus, tokenizer_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences_50[\"full\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5591"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_arr[\"full\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5591"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_arr[\"jane\"]) + len(corpus_arr[\"shakespeare\"]) + len(corpus_arr[\"lovecraft\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def get_X_Y(corpus_arr: dict, embedding: gensim.models.Word2Vec):\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    i = 0\n",
    "    count = 0\n",
    "    for author in books:\n",
    "        for sentence in corpus_arr[author]:\n",
    "            vector = None\n",
    "            for token in sentence:\n",
    "                token_vector = embedding.wv[token]\n",
    "                if vector is None:\n",
    "                    vector = embedding.wv[token]\n",
    "                else:\n",
    "                    vector = np.concatenate((vector, token_vector))\n",
    "            x_list.append(vector)\n",
    "            y_list.append(i)\n",
    "        i += 1\n",
    "\n",
    "    # for author in sequences:\n",
    "    #     if author != \"full\":\n",
    "    #         for sequence in sequences[author]:\n",
    "    #             vector = np.zeros((1, 150 * ))\n",
    "    #             x_list.append(pad_seq)\n",
    "    #             y_list.append(i)\n",
    "    #         i += 1\n",
    "\n",
    "    X = np.array(x_list)\n",
    "    Y = np.array(y_list)\n",
    "\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = get_X_Y(corpus_arr, embeding_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5591, 7500)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5591,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def get_train_test_val(X,Y):\n",
    "    \"\"\"\n",
    "    Returns n-tuple: (X_train, X_test, X_val, y_train, y_test, y_val)\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5)\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = get_train_test_val(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_embeding_matrix(embeding: gensim.models.Word2Vec, tokenizer: Tokenizer):\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    vector_size = embeding.layer1_size\n",
    "    matrix = np.zeros((vocab_size, vector_size))\n",
    "    for word in tokenizer.word_index:\n",
    "        i = tokenizer.word_index[word]\n",
    "        matrix[i] = embeding.wv[word]\n",
    "\n",
    "    return matrix\n",
    "\n",
    "embeding_matrix_50 = get_embeding_matrix(embeding_50, tokenizer_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "def get_model(input_size, num_dense = 2, inter_units = 100, dropout = 0.3, dense_units = 3) -> Sequential:\n",
    "    \"\"\"\n",
    "    Generates keras model.\n",
    "        Params:\n",
    "        -------\n",
    "            data: n-tuple\n",
    "                X_train, X_val, y_train, y_val\n",
    "\n",
    "            gensim_embeding\n",
    "                embeding generated using gensim\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape=(input_size)))\n",
    "    for i in range(num_dense):\n",
    "        model.add(Dense(inter_units, \"relu\"))\n",
    "        model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(3, \"softmax\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model_50 = get_model(X_train.shape[1], dropout=0.7)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 100)               750100    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 760503 (2.90 MB)\n",
      "Trainable params: 760503 (2.90 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_50.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_50.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "123/123 [==============================] - 3s 15ms/step - loss: 0.8160 - accuracy: 0.7424 - val_loss: 0.1253 - val_accuracy: 0.9595\n",
      "Epoch 2/7\n",
      "123/123 [==============================] - 2s 13ms/step - loss: 0.4334 - accuracy: 0.8717 - val_loss: 0.0538 - val_accuracy: 0.9857\n",
      "Epoch 3/7\n",
      "123/123 [==============================] - 2s 12ms/step - loss: 0.2952 - accuracy: 0.9080 - val_loss: 0.0505 - val_accuracy: 0.9809\n",
      "Epoch 4/7\n",
      "123/123 [==============================] - 2s 13ms/step - loss: 0.2133 - accuracy: 0.9295 - val_loss: 0.0476 - val_accuracy: 0.9809\n",
      "Epoch 5/7\n",
      "123/123 [==============================] - 2s 13ms/step - loss: 0.1915 - accuracy: 0.9410 - val_loss: 0.0399 - val_accuracy: 0.9893\n",
      "Epoch 6/7\n",
      "123/123 [==============================] - 2s 12ms/step - loss: 0.1332 - accuracy: 0.9479 - val_loss: 0.0363 - val_accuracy: 0.9905\n",
      "Epoch 7/7\n",
      "123/123 [==============================] - 2s 13ms/step - loss: 0.1395 - accuracy: 0.9509 - val_loss: 0.0402 - val_accuracy: 0.9905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x179bb447250>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_50.fit(X_train, y_train, validation_data = (X_val,y_val), epochs=7, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 3ms/step\n",
      "Informe de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       622\n",
      "           1       0.94      0.97      0.96       100\n",
      "           2       0.98      1.00      0.99       117\n",
      "\n",
      "    accuracy                           0.99       839\n",
      "   macro avg       0.97      0.99      0.98       839\n",
      "weighted avg       0.99      0.99      0.99       839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "y_pred = model_50.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Generar un informe de clasificación\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Informe de Clasificación:\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_TF_keras_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
